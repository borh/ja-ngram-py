#!/usr/bin/env python3
# Copyright 2017 Bor Hodošček

import argparse
import subprocess
import csv
import unicodedata
import re
import os.path
from collections import Counter, deque
from itertools import islice, chain


FEATURE_NAMES = ['sentence_boundary', 'orth', 'pron', 'form', 'lemma', 'pos', 'conj_type', 'conj_form', 'oov']

DELIMITERS = r'[!\?。！？]'
CLOSING_QUOTATIONS = r'[\)）」』】］〕〉》\]]'


def split_sentence_ja(s):
    """Splits Japanese sentence on common sentence delimiters. Some care is taken to prevent false positives, but more
    sophisticated (non-regex) logic would be required to implement a more robust solution."""
    return re.sub(r'({})(?!{})'.format(DELIMITERS, CLOSING_QUOTATIONS), r'\1\n', s).splitlines()


def run_mecab(fn, cleaned_fn, mecab_fn):
    """Shells out to mecab on PATH and creates a sentence-split and NFC normalized text file as well as the MeCab
    (-Ochamame) TSV file based on it."""
    with open(fn, 'r') as f:
        text = '\n'.join(line.rstrip() for line in f)

    lines = unicodedata.normalize('NFC', text).splitlines()
    
    sentences = [sentence for sentence in chain.from_iterable(split_sentence_ja(line) for line in lines)
                 if sentence]

    # print([sentence for sentence in sentences if len(sentence) > 400])

    with open(cleaned_fn, 'w') as f:
        f.write('\n'.join(sentences))

    output = subprocess.check_output(
        ['mecab', '-Ochamame', '-b' + str(2**32)],
        input=bytes('\n'.join(sentences), encoding='utf-8'),
    )

    with open(mecab_fn, 'w') as o:
        o.write('\t'.join(FEATURE_NAMES) + '\n')
    with open(mecab_fn, 'ab') as o:
        o.write(output)


def collapse_features(n_gram, fields, features_sep, n_gram_sep):
    """Concatenates both n-grams and the selected features of the morphemes into one string."""
    try:
        return n_gram_sep.join(features_sep.join(morpheme[field] for field in fields)
                               for morpheme in n_gram)
    except Exception as e:
        print('{} encountered while processing:\nn-gram: {}\nfields: {}'.format(e, n_gram, fields))


def n_gram_filter_pos(morphemes, filter_conditions):
    """Given a sequence of morphemes and POS filter rules (regular expressions), returns True if all morphemes match
    their positionally equivalent regular expression."""
    if not filter_conditions:
        return True

    for morpheme, condition in zip(morphemes, filter_conditions):
        if not condition.search(morpheme['pos']):
            return None
    return True


def n_gram_generator(iterable, n):
    """Generic n-gram extractor. Given an iterable, it will return a generator of n-grams with specified order n."""
    iterator = iter(iterable)
    d = deque(islice(iterator, n-1), maxlen=n)
    for item in iterator:
        d.append(item)
        yield tuple(d)


def extract_features(fn):
    """Extracts morphemes from specified CSV file generated by MeCab's -Ochamame output type (UniDic support only)."""
    with open(fn, 'r') as f:
        r = csv.DictReader(f,
                           fieldnames=FEATURE_NAMES,
                           delimiter='\t')
        for row in r:
            yield row


def generate_matrix(file_to_n_grams, min_freq=5, relative_freq=True, transpose=False):
    """Generates matrix with files representing rows and columns representing extracted n-gram features."""

    unique_n_grams = Counter()
    for _, n_grams in file_to_n_grams.items():
        unique_n_grams += n_grams

    unique_n_grams = [k for k, v in unique_n_grams.items() if v >= min_freq]

    if relative_freq:
        file_to_n_grams = {fn: Counter({k: (v / sum(n_grams.values()) * 1e6) for k, v in n_grams.items()})
                           for fn, n_grams in file_to_n_grams.items()}

    m = [[fn] + [n_grams[u] for u in unique_n_grams] for fn, n_grams in file_to_n_grams.items()]

    header = None
    if transpose:
        header = ['n_gram'] + [fn for fn, _ in file_to_n_grams.items()]
        freqs = [list(x) for x in zip(*[z[1:] for z in m])]
        m = [[u] + f for u, f in zip(unique_n_grams, freqs)]
    else:
        header = ['filename'] + unique_n_grams

    return header, m


def process_file(fn, features, features_sep, n_gram_sep):
    """Main processing function that sets up parameters and wires all logic needed to generate specified n-gram
    features."""
    print('Processing \'{}\'...'.format(fn))

    filter_conditions = args['positional_pos_filter']
    if filter_conditions:
        filter_conditions = [re.compile(c) for c in filter_conditions]

    cleaned_fn = '{}.txt.cleaned'.format(os.path.splitext(os.path.basename(fn))[0])
    mecab_fn = '{}-mecab.tsv'.format(os.path.splitext(os.path.basename(fn))[0])

    run_mecab(fn, cleaned_fn, mecab_fn)

    morphemes = extract_features(mecab_fn)

    n_grams = n_gram_generator(morphemes, n=args['n'])

    filtered_n_grams = Counter(collapse_features(n_gram, features, features_sep, n_gram_sep)
                               for n_gram in n_grams
                               if n_gram_filter_pos(n_gram, filter_conditions=filter_conditions))

    return fn, filtered_n_grams


def parse_args():
    """Parses and return program arguments as dictionary."""
    parser = argparse.ArgumentParser(description='Extracts n-grams using MeCab (UniDic) from multiple input files and '
                                                 'outputs an n-gram frequency matrix by file. Supports arbitrary '
                                                 'filtering of POS sequences using regular expressions.')
    parser.add_argument('--n',
                        nargs='?',
                        help='specify the n-gram order (default=2)',
                        default=2,
                        type=int,
                        required=False)
    parser.add_argument('--n_gram-sep',
                        nargs='?',
                        help='specify the separator used to separate n-grams (default=\'_\')',
                        default='_',
                        required=False)
    parser.add_argument('--min-freq',
                        nargs='?',
                        help='specify the minimum n-gram frequency of tokens to extract (default=5)',
                        default=5,
                        type=int,
                        required=False)
    parser.add_argument('--relative-freq',
                        help='specify if frequency counts should be normalized to parts-per-million (default=True)',
                        action='store_true',
                        default=False,
                        required=False)
    parser.add_argument('--positional-pos-filter',
                        nargs='+',
                        help='specify consecutive regular expressions to match to the POS fields of tokens '
                             '(Ex. \'.*\' \'記号\' to match any POS followed by a symbol)',
                        required=False)
    parser.add_argument('--features',
                        nargs='+',
                        help='specify which features should be extracted from morphemes (default is \'orth\' \'pos\')',
                        default=['orth', 'pos'],
                        required=False)
    parser.add_argument('--features-sep',
                        nargs='?',
                        help='specify the separator used to separate features (default=\'/\')',
                        default='/',
                        required=False)
    parser.add_argument('--transpose',
                        help='specify whether to transpose the output matrix or not (default=False)',
                        action='store_true',
                        default=False,
                        required=False)
    parser.add_argument('--in',
                        nargs='+',
                        help='one or more input file(s) (must be plain text formatted UTF-8 files)',
                        required=True)
    parser.add_argument('--out',
                        help='output file as n-gram frequency count matrix',
                        required=True)

    return vars(parser.parse_args())

if __name__ == '__main__':
    args = parse_args()

    processed_files = dict(process_file(fn,
                                        args['features'],
                                        args['features_sep'],
                                        args['n_gram_sep'])
                          for fn in args['in'])

    header, output_matrix = generate_matrix(processed_files,
                                            min_freq=args['min_freq'],
                                            relative_freq=args['relative_freq'],
                                            transpose=args['transpose'])

    with open(args['out'], 'w', newline='') as f:
        w = csv.writer(f)
        w.writerow(header)
        w.writerows(output_matrix)
